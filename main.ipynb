{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from DQN import neural_net, DQN, lr, MEMORY_SIZE\n",
    "from atari_wrappers import modify_env\n",
    "from replay_memory import ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "env = gym.make(DEFAULT_ENV_NAME)\n",
    "env = modify_env(env)\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create networks\n",
    "# if the file exists, load the model\n",
    "if os.path.exists(\"dqn_pong_model\"):\n",
    "    policy_net = torch.load(\"dqn_pong_model\")\n",
    "else:\n",
    "    policy_net = neural_net(n_actions=4).to(device)\n",
    "target_net = neural_net(n_actions=4).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "# initialize replay memory\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "# train model\n",
    "model = DQN(policy_net, target_net, optimizer, memory, device, n_actions=4)\n",
    "model.train(env, 3000)\n",
    "torch.save(model.policy_net, \"dqn_pong_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "policy_net = torch.load(\"dqn_pong_model\")\n",
    "model.test(env, 1, policy_net, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_test_render(env):\n",
    "    # run random policy and render\n",
    "    env.reset()\n",
    "    for _ in range(1000):\n",
    "        fig = plt.figure()\n",
    "        arr = env.render(mode='rgb_array')\n",
    "        plt.imshow(arr)\n",
    "        plt.show()\n",
    "        env.step(env.action_space.sample())\n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot()\n",
    "model.plot(logarithmic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_steps = len(model.log['steps_done'])\n",
    "array = np.zeros((n_steps, 5))\n",
    "array[:, 0] = model.log['steps_done']\n",
    "array[:, 1] = model.log['episode']\n",
    "array[:, 2] = model.log['reward']\n",
    "array[:, 3] = model.log['total_reward']\n",
    "array[:, 4] = model.log['loss']\n",
    "np.savetxt('dqn_pong_model.csv', array, delimiter=',', header='steps_done,episode,reward,total_reward,loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ae-598-rl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
